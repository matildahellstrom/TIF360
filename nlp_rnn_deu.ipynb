{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating with Recurrent Neural Networks\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f0f8ff; border: 2px solid #4682b4; padding: 10px;\">\n",
    "<a href=\"https://colab.research.google.com/github/DeepTrackAI/DeepLearningCrashCourse/blob/main/Ch07_RNN/ec07_A_nlp_rnn/nlp_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<strong>If using Colab/Kaggle:</strong> You need to uncomment the code in the cell below this one.\n",
    "You need also to copy the \"eng-spa.txt\" file from the\n",
    "<a href=\"https://github.com/DeepTrackAI/DeepLearningCrashCourse/tree/main/Ch07_RNN/ec07_A_nlp_rnn\">notebook folder</a> in GitHub to the Colab/Kaggle work directory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if using Colab/Kaggle.\n",
    "!pip install contractions deeplay deeptrack spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides you with a complete code example that implements a sequence-to-sequence (seq2seq) model for machine translation using recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f8ff; border: 2px solid #4682b4; padding: 10px;\">\n",
    "<strong>Note:</strong> This notebook contains the Code Example 7-A from the book  \n",
    "\n",
    "**Deep Learning Crash Course**  \n",
    "Benjamin Midtvedt, Jesús Pineda, Henrik Klein Moberg, Harshith Bachimanchi, Joana B. Pereira, Carlo Manzo, Giovanni Volpe  \n",
    "No Starch Press, San Francisco (CA), 2025  \n",
    "ISBN-13: 9781718503922  \n",
    "\n",
    "[https://nostarch.com/deep-learning-crash-course](https://nostarch.com/deep-learning-crash-course)\n",
    "\n",
    "You can find the other notebooks on the [Deep Learning Crash Course GitHub page](https://github.com/DeepTrackAI/DeepLearningCrashCourse).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Bilingual Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Sentences\n",
    "\n",
    "Implement a function to tokenize a sentence ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "tokenizers = {\"eng\": spacy.blank(\"en\"), \"deu\": spacy.blank(\"de\")}\n",
    "\n",
    "def tokenize(text, lang=\"eng\"):\n",
    "    \"\"\"Tokenize text.\"\"\"\n",
    "    tokens = tokenizers[lang](text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'simple', 'example', '!']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in tokenize(\"This is a simple example!\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then update this function to handle contractions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions, spacy\n",
    "\n",
    "tokenizers = {\"eng\": spacy.blank(\"en\"), \"deu\": spacy.blank(\"de\")}\n",
    "\n",
    "def tokenize(text, lang=\"eng\"):\n",
    "    \"\"\"Tokenize text.\"\"\"\n",
    "    text = contractions.fix(text) if lang == \"eng\" else text\n",
    "    tokens = tokenizers[lang](text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'not', 'the', 'same', 'example', '!']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in tokenize(\"This isn't the same example!\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then update this function to remove irrelevant punctuation and non-alphabetical characters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions, re, spacy, unicodedata\n",
    "\n",
    "tokenizers = {\"eng\": spacy.blank(\"en\"), \"deu\": spacy.blank(\"de\")}\n",
    "\n",
    "regular_expression = r\"^[a-zA-Z0-9áéíóúüñÁÉÍÓÚÜÑ.,!?¡¿/:()]+$\"\n",
    "pattern = re.compile(unicodedata.normalize(\"NFC\", regular_expression))\n",
    "\n",
    "def tokenize(text, lang=\"eng\"):\n",
    "    \"\"\"Tokenize text.\"\"\"\n",
    "    swaps = {\"’\": \"'\", \"‘\": \"'\", \"“\": '\"', \"”\": '\"', \"´\": \"'\", \"´´\": '\"'}\n",
    "    for old, new in swaps.items():\n",
    "        text = text.replace(old, new)\n",
    "    text = contractions.fix(text) if lang == \"eng\" else text\n",
    "    tokens = tokenizers[lang](text)\n",
    "    return [token.text for token in tokens if pattern.match(token.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Double', 'check', 'your', 'code', '!']\n"
     ]
    }
   ],
   "source": [
    "print([token for token in tokenize(\"Double-check your code!\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Corpus Iterator\n",
    "\n",
    "Implement a function to read and tokenize sentences by iterating through a corpus file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_iterator(filename, lang, lang_position):\n",
    "    \"\"\"Read and tokenize texts by iterating through a corpus file.\"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            sentences = line.strip().split(\"\\t\")\n",
    "            sentence = unicodedata.normalize(\"NFC\", sentences[lang_position])\n",
    "            yield tokenize(sentence, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and use it to extract the English and corresponding Spanish tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens_eng, tokens_spa in zip(\n",
    "    corpus_iterator(filename=\"deu.txt\", lang=\"eng\", lang_position=0),\n",
    "    corpus_iterator(filename=\"deu.txt\", lang=\"deu\", lang_position=1),\n",
    "    ):\n",
    "    print(f\"{tokens_eng} {tokens_spa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Vocabulary\n",
    "\n",
    "Implement a class to represent a vocabulary ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary as callable dictionary.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_dict, unk_token=\"<unk>\"):\n",
    "        \"\"\"Initialize vocabulary.\"\"\"\n",
    "        self.vocab_dict, self.unk_token = vocab_dict, unk_token\n",
    "        self.default_index = vocab_dict.get(unk_token, -1)\n",
    "        self.index_to_token = {idx: token for token, idx in vocab_dict.items()}\n",
    "\n",
    "    def __call__(self, token_or_tokens):\n",
    "        \"\"\"Return the index(es) for given token or list of tokens.\"\"\"\n",
    "        if not isinstance(token_or_tokens, list):\n",
    "            return self.vocab_dict.get(token_or_tokens, self.default_index)\n",
    "        else:\n",
    "            return [self.vocab_dict.get(token, self.default_index)\n",
    "                    for token in token_or_tokens]\n",
    "\n",
    "    def set_default_index(self, index):\n",
    "        \"\"\"Set default index for unknown tokens.\"\"\"\n",
    "        self.default_index = index\n",
    "\n",
    "    def lookup_token(self, index_or_indices):\n",
    "        \"\"\"Retrieve token corresponding to given index or list of indices.\"\"\"\n",
    "        if not isinstance(index_or_indices, list):\n",
    "            return self.index_to_token.get(int(index_or_indices),\n",
    "                                           self.unk_token)\n",
    "        else:\n",
    "            return [self.index_to_token.get(int(index), self.unk_token)\n",
    "                    for index in index_or_indices]\n",
    "\n",
    "    def get_tokens(self):\n",
    "        \"\"\"Return a list of tokens ordered by their index.\"\"\"\n",
    "        tokens = [None] * len(self.index_to_token)\n",
    "        for index, token in self.index_to_token.items():\n",
    "            tokens[index] = token\n",
    "        return tokens\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate over the tokens in the vocabulary.\"\"\"\n",
    "        return iter(self.vocab_dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of tokens in the vocabulary.\"\"\"\n",
    "        return len(self.vocab_dict)\n",
    "\n",
    "    def __contains__(self, token):\n",
    "        \"\"\"Check if a token is in the vocabulary.\"\"\"\n",
    "        return token in self.vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which you can use as shown ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {\"hello\": 0, \"world\": 1, \"<unk>\": 2}\n",
    "vocab = Vocab(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'world'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement a function to build vocabulary from an iterator ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab_from_iterator(iterator, specials=None, min_freq=1):\n",
    "    \"\"\"Build vocabulary from an iterator over tokenized sentences.\"\"\"\n",
    "    token_freq = Counter(token for tokens in iterator for token in tokens)\n",
    "    vocab, index = {}, 0\n",
    "    if specials:\n",
    "        for token in specials:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    for token, freq in token_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which you can then use on a list of tokenized sentences ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [[\"this\", \"is\", \"an\", \"example\"],\n",
    "                       [\"another\", \"example\", \"sentence\"],\n",
    "                       [\"this\", \"is\", \"a\", \"test\"]]\n",
    "vocab_dict = build_vocab_from_iterator(\n",
    "    tokenized_sentences, specials=[\"<unk>\", \"<pad>\"], min_freq=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, '<pad>': 1, 'this': 2, 'is': 3, 'an': 4, 'example': 5, 'another': 6, 'sentence': 7, 'a': 8, 'test': 9}\n"
     ]
    }
   ],
   "source": [
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement a function to build a vocabulary from a corpus file ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filename, lang, lang_position, specials=[\"<unk>\"], min_freq=5):\n",
    "    \"\"\"Build vocabulary.\"\"\"\n",
    "    vocab_dict = build_vocab_from_iterator(\n",
    "        corpus_iterator(filename, lang, lang_position), specials, min_freq,\n",
    "    )\n",
    "    vocab = Vocab(vocab_dict, unk_token=specials[0])\n",
    "    vocab.set_default_index(vocab(specials[0]))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and use this function to create the vocabularies for the input and output vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_lang, out_lang, filename = \"eng\", \"deu\", \"deu.txt\"\n",
    "specials = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "\n",
    "in_vocab = build_vocab(filename, in_lang, lang_position=0, specials=specials)\n",
    "out_vocab = build_vocab(filename, out_lang, lang_position=1, specials=specials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "\n",
    "Implement a function to check if all words in a sentence are present in a vocabulary ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_words_in_vocab(sentence, vocab):\n",
    "    \"\"\"Check whether all words in a sentence are present in a vocabulary.\"\"\"\n",
    "    return all(word in vocab for word in sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to pad a sequence of tokens ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tokens, max_length=10):\n",
    "    \"\"\"Pad sequence of tokens.\"\"\"\n",
    "    padding_length = max_length - len(tokens)\n",
    "    return [\"<sos>\"] + tokens + [\"<eos>\"] + [\"<pad>\"] * padding_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to process the language corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process(filename, in_lang, out_lang, in_vocab, out_vocab, max_length=10):\n",
    "    \"\"\"Process language corpus.\"\"\"\n",
    "    in_sequences, out_sequences = [], []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            sentences = line.strip().split(\"\\t\")\n",
    "            in_tokens = tokenize(unicodedata.normalize(\"NFC\", sentences[0]),\n",
    "                                 in_lang)\n",
    "            out_tokens = tokenize(unicodedata.normalize(\"NFC\", sentences[1]),\n",
    "                                  out_lang)\n",
    "\n",
    "            if (all_words_in_vocab(in_tokens, in_vocab)\n",
    "                and len(in_tokens) <= max_length\n",
    "                and all_words_in_vocab(out_tokens, out_vocab)\n",
    "                and len(out_tokens) <= max_length):\n",
    "\n",
    "                padded_in_tokens = pad(in_tokens)\n",
    "                in_sequence = in_vocab(padded_in_tokens)\n",
    "                in_sequences.append(in_sequence)\n",
    "\n",
    "                padded_out_tokens = pad(out_tokens)\n",
    "                out_sequence = out_vocab(padded_out_tokens)\n",
    "                out_sequences.append(out_sequence)\n",
    "    return np.array(in_sequences), np.array(out_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and build the datasets and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pint.util:Redefining '[magnetic_flux]' (<class 'pint.delegates.txt_defparser.plain.DerivedDimensionDefinition'>)\n"
     ]
    }
   ],
   "source": [
    "import deeplay as dl\n",
    "import deeptrack as dt\n",
    "import torch\n",
    "\n",
    "in_sequences, out_sequences = \\\n",
    "    process(filename, in_lang, out_lang, in_vocab, out_vocab)\n",
    "\n",
    "sources = dt.sources.Source(inputs=in_sequences, targets=out_sequences)\n",
    "train_sources, test_sources = dt.sources.random_split(sources, [0.90, 0.10]) #changed from 85 15\n",
    "\n",
    "inputs_pip = dt.Value(sources.inputs) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "outputs_pip = dt.Value(sources.targets) >> dt.pytorch.ToTensor(dtype=torch.int)\n",
    "\n",
    "train_dataset = \\\n",
    "    dt.pytorch.Dataset(inputs_pip & outputs_pip, inputs=train_sources)\n",
    "test_dataset = \\\n",
    "    dt.pytorch.Dataset(inputs_pip & outputs_pip, inputs=test_sources)\n",
    "\n",
    "train_loader = dl.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = dl.DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing and Training the Sequence-to-Sequence Architecture\n",
    "\n",
    "Implement the encoder ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(dl.DeeplayModule):\n",
    "    \"\"\"Sequence-to-sequence encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, in_feats=300, hidden_feats=128,\n",
    "                 hidden_layers=1, dropout=0.0):\n",
    "        \"\"\"Initialize sequence-to-sequence encoder.\"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_feats, self.hidden_layers = hidden_feats, hidden_layers\n",
    "\n",
    "        self.embedding = dl.Layer(torch.nn.Embedding, vocab_size, in_feats)\n",
    "        self.rnn = dl.Layer(torch.nn.GRU, input_size=in_feats,\n",
    "                            hidden_size=hidden_feats, num_layers=hidden_layers,\n",
    "                            dropout=(0 if hidden_layers == 1 else dropout),\n",
    "                            bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, in_sequences, contexts=None):\n",
    "        \"\"\"Calculate the encoded sequences and contexts.\"\"\"\n",
    "        in_embeddings = self.embedding(in_sequences)\n",
    "        encoded_sequences, contexts = self.rnn(in_embeddings, contexts)\n",
    "        encoded_sequences = (encoded_sequences[:, :, :self.hidden_feats]\n",
    "                             + encoded_sequences[:, :, self.hidden_feats:])\n",
    "        contexts = contexts[:self.hidden_layers]\n",
    "        return encoded_sequences, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement the decoder ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(dl.DeeplayModule):\n",
    "    \"\"\"Sequence-to-sequence decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, in_feats=300, hidden_feats=128,\n",
    "                 hidden_layers=1, dropout=0.0):\n",
    "        \"\"\"Initialize sequence-to-sequence decoder.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = dl.Layer(torch.nn.Embedding, vocab_size, in_feats)\n",
    "        self.rnn = dl.Layer(torch.nn.GRU, input_size=in_feats,\n",
    "                            hidden_size=hidden_feats, num_layers=hidden_layers,\n",
    "                            bidirectional=False, batch_first=True,\n",
    "                            dropout=(0 if hidden_layers == 1 else dropout))\n",
    "        self.dense = dl.Layer(torch.nn.Linear, hidden_feats, vocab_size)\n",
    "        self.softmax = dl.Layer(torch.nn.Softmax, dim=-1)\n",
    "\n",
    "    def forward(self, decoder_in_values, contexts):\n",
    "        \"\"\"Calculate the decoder outputs and contexts.\"\"\"\n",
    "        out_embeddings = self.embedding(decoder_in_values)\n",
    "        decoder_outputs, contexts = self.rnn(out_embeddings, contexts)\n",
    "        decoder_outputs = self.dense(decoder_outputs)\n",
    "        decoder_outputs = self.softmax(decoder_outputs)\n",
    "        return decoder_outputs, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement the full seq2seq model combining the encoder and decoder ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(dl.DeeplayModule):\n",
    "    \"\"\"Sequence-to-sequence model with evaluation method.\"\"\"\n",
    "\n",
    "    def __init__(self, in_vocab_size=None, out_vocab_size=None, embed_dim=300,\n",
    "                 hidden_feats=128, hidden_layers=1, dropout=0.0,\n",
    "                 teacher_prob=1.0):\n",
    "        \"\"\"Initialize the sequence-to-sequence model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.in_vocab_size, self.out_vocab_size = in_vocab_size, out_vocab_size\n",
    "        self.teacher_prob = teacher_prob\n",
    "\n",
    "        self.encoder = Seq2SeqEncoder(in_vocab_size, embed_dim, hidden_feats,\n",
    "                                      hidden_layers, dropout)\n",
    "        self.decoder = Seq2SeqDecoder(out_vocab_size, embed_dim, hidden_feats,\n",
    "                                      hidden_layers, dropout)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Calculate the decoder output vectors for the input sequences.\"\"\"\n",
    "        in_sequences, out_sequences = batch\n",
    "        num_sequences, sequence_length = in_sequences.size()\n",
    "        device = next(self.encoder.parameters()).device\n",
    "\n",
    "        _, contexts = self.encoder(in_sequences)\n",
    "\n",
    "        decoder_outputs_vec = torch.zeros(num_sequences, sequence_length,\n",
    "                                          self.out_vocab_size).to(device)\n",
    "        decoder_in_values = torch.full(size=(num_sequences, 1),\n",
    "                                       fill_value=1, device=device)  # <sos>\n",
    "        for t in range(sequence_length):\n",
    "            decoder_outputs, contexts = \\\n",
    "                self.decoder(decoder_in_values, contexts)\n",
    "            decoder_outputs_vec[:, t, :] = decoder_outputs.squeeze(1)\n",
    "\n",
    "            if (np.random.rand() < self.teacher_prob\n",
    "                and t < sequence_length - 1):  # Teacher forcing.\n",
    "                decoder_in_values = \\\n",
    "                    out_sequences[:, t + 1].unsqueeze(-1).to(device)\n",
    "            else:  # Model prediction.\n",
    "                _, top_decoder_outputs = decoder_outputs.topk(1)\n",
    "                decoder_in_values = \\\n",
    "                    top_decoder_outputs.squeeze(-1).detach().to(device)\n",
    "        \n",
    "        return decoder_outputs_vec\n",
    "\n",
    "    def evaluate(self, in_sequences):\n",
    "        \"\"\"Evaluate model.\"\"\"\n",
    "        num_sequences, sequence_length = in_sequences.size()\n",
    "        device = next(self.encoder.parameters()).device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, contexts = self.encoder(in_sequences)\n",
    "\n",
    "        pred_sequences = torch.zeros(num_sequences, sequence_length).to(device)\n",
    "        decoder_in_values = torch.full(size=(num_sequences, 1),\n",
    "                                       fill_value=1, device=device)  # <sos>\n",
    "        for t in range(sequence_length):\n",
    "            with torch.no_grad():\n",
    "                decoder_outputs, contexts = \\\n",
    "                    self.decoder(decoder_in_values, contexts)\n",
    "            _, top_decoder_outputs = decoder_outputs.topk(1)\n",
    "            pred_sequences[:, t] = top_decoder_outputs.squeeze()\n",
    "\n",
    "            decoder_in_values = top_decoder_outputs.squeeze(-1).detach()\n",
    "\n",
    "        return pred_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... define the loss function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskedNLL(decoder_outputs, out_sequences, padding=0):\n",
    "    \"\"\"Calculate the masked negative log-likelihood (NLL) loss.\"\"\"\n",
    "    flat_pred_sequences = decoder_outputs.view(-1, decoder_outputs.shape[-1])\n",
    "    flat_target_sequences = out_sequences.view(-1, 1)\n",
    "    pred_probs = torch.gather(flat_pred_sequences, 1, flat_target_sequences)\n",
    "\n",
    "    nll = - torch.log(pred_probs)\n",
    "\n",
    "    mask = out_sequences != padding\n",
    "    masked_nll = nll.masked_select(mask.view(-1, 1))\n",
    "\n",
    "    return masked_nll.mean()  # Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and implement the sequence-to-sequence application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(dl.Application):\n",
    "    \"\"\"Application for the sequence-to-sequence model.\"\"\"\n",
    "\n",
    "    def __init__(self, in_vocab, out_vocab, teacher_prob=1.0):\n",
    "        \"\"\"Initialize the application.\"\"\"\n",
    "        super().__init__(loss=maskedNLL, optimizer=dl.Adam(lr=1e-3))\n",
    "        self.model = Seq2SeqModel(in_vocab_size=len(in_vocab),\n",
    "                                  out_vocab_size=len(out_vocab),\n",
    "                                  teacher_prob=teacher_prob)\n",
    "\n",
    "    def train_preprocess(self, batch):\n",
    "        \"\"\"Adjust the target sequence by shifting it one position backward.\"\"\"\n",
    "        in_sequences, out_sequences = batch\n",
    "        shifted_out_sequences = \\\n",
    "            torch.cat((out_sequences[:, 1:], out_sequences[:, -1:]), dim=1)\n",
    "        return (in_sequences, out_sequences), shifted_out_sequences\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Perform forward pass.\"\"\"\n",
    "        return self.model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Embeddings\n",
    "\n",
    "Download the GloVe embeddings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets.utils import download_url, extract_archive\n",
    "\n",
    "glove_folder = \".glove_cache\"\n",
    "if not os.path.exists(glove_folder):\n",
    "    os.makedirs(glove_folder, exist_ok=True)\n",
    "    url = \"https://nlp.stanford.edu/data/glove.42B.300d.zip\"\n",
    "    download_url(url, glove_folder)\n",
    "    zip_filepath = os.path.join(glove_folder, \"glove.42B.300d.zip\")\n",
    "    extract_archive(zip_filepath, glove_folder)\n",
    "    os.remove(zip_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement a function to load the GloVe embeddings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_file):\n",
    "    \"\"\"Load GloVe embeddings.\"\"\"\n",
    "    glove_embeddings = {}\n",
    "    with open(glove_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            glove_embeddings[word] = np.round(\n",
    "                np.asarray(values[1:], dtype=\"float32\"), decimals=6,\n",
    "            )\n",
    "    return glove_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... implement a function to get GloVe embeddings for a vocabulary ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embeddings(vocab, glove_embeddings, embed_dim):\n",
    "    \"\"\"Get GloVe embeddings for a vocabulary.\"\"\"\n",
    "    embeddings = torch.zeros((len(vocab), embed_dim), dtype=torch.float32)\n",
    "    for i, token in enumerate(vocab):\n",
    "        embedding = glove_embeddings.get(token)\n",
    "        if embedding is None:\n",
    "            embedding = glove_embeddings.get(token.lower())\n",
    "        if embedding is not None:\n",
    "            embeddings[i] = torch.tensor(embedding, dtype=torch.float32)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... load the pretrained GloVe embeddings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = os.path.join(glove_folder, \"glove.42B.300d.txt\")\n",
    "glove_embeddings, glove_dim = load_glove_embeddings(glove_file), 300\n",
    "\n",
    "embeddings_in = get_glove_embeddings(in_vocab.get_tokens(),\n",
    "                                     glove_embeddings, glove_dim)\n",
    "embeddings_out = get_glove_embeddings(out_vocab.get_tokens(),\n",
    "                                      glove_embeddings, glove_dim)\n",
    "\n",
    "num_specials = len(specials)\n",
    "embeddings_in[1:num_specials] = torch.rand(num_specials - 1, glove_dim) * 0.01\n",
    "embeddings_out[1:num_specials] = torch.rand(num_specials - 1, glove_dim) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Sequence-to-Sequence Application\n",
    "\n",
    "Create the seq2seq model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(in_vocab=in_vocab, out_vocab=out_vocab, teacher_prob=0.85)\n",
    "seq2seq = seq2seq.create()\n",
    "\n",
    "seq2seq.model.encoder.embedding.weight.data = embeddings_in\n",
    "seq2seq.model.encoder.embedding.weight.requires_grad = False\n",
    "seq2seq.model.decoder.embedding.weight.data = embeddings_out\n",
    "seq2seq.model.decoder.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matilda/Desktop/TIF360/deeplearn-py310/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/Users/matilda/Desktop/TIF360/deeplearn-py310/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | train_metrics | MetricCollection | 0      | train\n",
      "1 | val_metrics   | MetricCollection | 0      | train\n",
      "2 | test_metrics  | MetricCollection | 0      | train\n",
      "3 | model         | Seq2SeqModel     | 7.7 M  | train\n",
      "4 | optimizer     | Adam             | 0      | train\n",
      "-----------------------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "5.8 M     Non-trainable params\n",
      "7.7 M     Total params\n",
      "30.904    Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/matilda/Desktop/TIF360/deeplearn-py310/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|███████████████████████████████████████████| 697/697 [01:22<00:00,  8.41it/s, v_num=182, train_loss_step=0.921, train_loss_epoch=0.830]\n"
     ]
    }
   ],
   "source": [
    "trainer = dl.Trainer(max_epochs=75, accelerator=\"auto\")\n",
    "trainer.fit(seq2seq, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model Perfomance\n",
    "\n",
    "Implement a function to convert numerical sequences into their corresponding text ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unprocess(sequences, vocab, specials):\n",
    "    \"\"\"Convert numeric sequences to sentences.\"\"\"\n",
    "    sentences = []\n",
    "    for sequence in sequences:\n",
    "        idxs = sequence[sequence > len(specials) - 1]\n",
    "        words = [vocab.lookup_token(idx) for idx in idxs]\n",
    "        sentences.append(\" \".join(words))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... a function to translate user-defined sentences ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(in_sentence, model, in_lang, in_vocab, out_vocab, specials):\n",
    "    \"\"\"Translate a sentence.\"\"\"\n",
    "    in_sentence = unicodedata.normalize(\"NFC\", in_sentence)\n",
    "    in_tokens = pad(tokenize(in_sentence, in_lang))\n",
    "    in_sequence = (torch.tensor(in_vocab(in_tokens), dtype=torch.int)\n",
    "                   .unsqueeze(0).to(next(model.parameters()).device))\n",
    "    pred_sequence = model.evaluate(in_sequence)\n",
    "    pred_sentence = unprocess(pred_sequence, out_vocab, specials)\n",
    "    print(f\"Predicted Translation: {pred_sentence[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... try to translate a simple sentence ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: Ich habe ein Buch gekauft .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_sentence = \"I bought a book.\"\n",
    "translate(in_sentence, seq2seq.model, in_lang, in_vocab, out_vocab, specials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... another simple sentence ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: Dieses Buch ist sehr interessant .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_sentence = \"This book is very interesting.\"\n",
    "translate(in_sentence, seq2seq.model, in_lang, in_vocab, out_vocab, specials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a more complex one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Translation: Der Film hat mir sehr gut gekauft .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_sentence = \"The book that I bought is very interesting.\"\n",
    "translate(in_sentence, seq2seq.model, in_lang, in_vocab, out_vocab, specials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model with the BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: Ten years have gone by .\n",
      "Predicted Translation: Jahre sind Jahre alt .\n",
      "Actual Translation: Zehn Jahre sind vergangen .\n",
      "\n",
      "Input Sentence: Tom is going to dump Mary .\n",
      "Predicted Translation: Tom wird Maria anrufen .\n",
      "Actual Translation: Tom wird Mary den Laufpass geben .\n",
      "\n",
      "Input Sentence: What is the password ?\n",
      "Predicted Translation: Was ist die Schlüssel ?\n",
      "Actual Translation: Wie lautet das Passwort ?\n",
      "\n",
      "Input Sentence: Do not ask me to do this .\n",
      "Predicted Translation: Frag mich nicht dazu , das zu tun !\n",
      "Actual Translation: Bitte mich nicht darum !\n",
      "\n",
      "Input Sentence: We have only got one left .\n",
      "Predicted Translation: Wir haben nur noch einen übrig .\n",
      "Actual Translation: Wir haben nur noch eine übrig .\n",
      "\n",
      "Input Sentence: I know you are tired .\n",
      "Predicted Translation: Ich , dass du müde bist .\n",
      "Actual Translation: Ich , dass du müde bist .\n",
      "\n",
      "Input Sentence: Tom was not a teacher .\n",
      "Predicted Translation: Tom war kein Lehrer .\n",
      "Actual Translation: Tom war kein Lehrer .\n",
      "\n",
      "Input Sentence: You looked like you were puzzled .\n",
      "Predicted Translation: Ihr erstaunt scheint Angst zu haben .\n",
      "Actual Translation: Du sahst so aus , als ob du verwirrt .\n",
      "\n",
      "Input Sentence: No one else was hurt .\n",
      "Predicted Translation: Sonst war niemand verletzt .\n",
      "Actual Translation: Sonst wurde niemand verletzt .\n",
      "\n",
      "Input Sentence: I felt something touch my feet .\n",
      "Predicted Translation: Ich fühlte mir etwas in meine Zehen .\n",
      "Actual Translation: Ich fühlte , wie etwas meine berührte .\n",
      "\n",
      "Input Sentence: I am not feeling well .\n",
      "Predicted Translation: Mir geht es nicht gut .\n",
      "Actual Translation: Ich fühle mich nicht gut .\n",
      "\n",
      "Input Sentence: Please hurry .\n",
      "Predicted Translation: Bitte beeilen Sie .\n",
      "Actual Translation: Beeilt euch bitte !\n",
      "\n",
      "Input Sentence: Did you ever do that ?\n",
      "Predicted Translation: Hast du das jemals getan ?\n",
      "Actual Translation: Haben Sie das schon einmal gemacht ?\n",
      "\n",
      "Input Sentence: It was very dark inside the room .\n",
      "Predicted Translation: Es war sehr dunkel fürs Zimmer .\n",
      "Actual Translation: Im Zimmer war es sehr dunkel .\n",
      "\n",
      "Input Sentence: That is a plan .\n",
      "Predicted Translation: Das ist ein Plan .\n",
      "Actual Translation: Das ist ein Plan .\n",
      "\n",
      "Input Sentence: I like to ride my bike .\n",
      "Predicted Translation: Ich fahre gerne Rad .\n",
      "Actual Translation: Ich fahre gern Rad .\n",
      "\n",
      "Input Sentence: Did you see the rainbow ?\n",
      "Predicted Translation: Hast du den Regenbogen gesehen ?\n",
      "Actual Translation: Hast du den Regenbogen gesehen ?\n",
      "\n",
      "Input Sentence: My legal name is Tom Jackson .\n",
      "Predicted Translation: Meine Toms Name ist .\n",
      "Actual Translation: Mein richtiger Name ist Tom Jackson .\n",
      "\n",
      "Input Sentence: Does this cap belong to you ?\n",
      "Predicted Translation: dir diese Stadt ?\n",
      "Actual Translation: diese Mütze dir ?\n",
      "\n",
      "Input Sentence: Tom lost his only son in a car accident .\n",
      "Predicted Translation: Tom hat seinen Sohn einen Schlüssel verloren .\n",
      "Actual Translation: Tom hat seinen einzigen Sohn bei einem Verkehrsunfall verloren .\n",
      "\n",
      "Input Sentence: That is why I was late for class yesterday .\n",
      "Predicted Translation: Deshalb bin ich am ersten Morgen gewesen .\n",
      "Actual Translation: Deswegen kam ich gestern zu zum Unterricht .\n",
      "\n",
      "Input Sentence: You are too late .\n",
      "Predicted Translation: Du bist zu zu .\n",
      "Actual Translation: Du kommst zu .\n",
      "\n",
      "Input Sentence: Tom phoned to say he could not come .\n",
      "Predicted Translation: Tom beeilte sich , , nicht er zu kommen .\n",
      "Actual Translation: Tom teilte telefonisch mit , nicht kommen zu .\n",
      "\n",
      "Input Sentence: Tom loves doing this .\n",
      "Predicted Translation: Tom macht das gerne .\n",
      "Actual Translation: Tom liebt es , das zu tun .\n",
      "\n",
      "Input Sentence: That hat matches the suit .\n",
      "Predicted Translation: Der Hut hat den Anzug aus .\n",
      "Actual Translation: Der Hut passt zu dem Anzug .\n",
      "\n",
      "Input Sentence: Tom advised him not to buy the secondhand car .\n",
      "Predicted Translation: Tom riet ihm , ihm kein Interesse zu leihen .\n",
      "Actual Translation: Tom riet ihm , den Gebrauchtwagen nicht zu kaufen .\n",
      "\n",
      "Input Sentence: He said that you had better go .\n",
      "Predicted Translation: Er sagte , du solltest besser gehen .\n",
      "Actual Translation: Er sagte , ihr solltet lieber gehen .\n",
      "\n",
      "Input Sentence: It was a camera that she bought there .\n",
      "Predicted Translation: Ich war ein , für sie alle Eintrittskarten .\n",
      "Actual Translation: Sie hat sich dort einen Fotoapparat gekauft .\n",
      "\n",
      "Input Sentence: Does Tom think that I can help ?\n",
      "Predicted Translation: Tom , dass ich helfen ?\n",
      "Actual Translation: Glaubt Tom , dass ich helfen kann ?\n",
      "\n",
      "Input Sentence: Tom never got caught .\n",
      "Predicted Translation: Tom wurde nie erwischt .\n",
      "Actual Translation: Tom wurde nie erwischt .\n",
      "\n",
      "Input Sentence: The woman suspected that her son was using drugs .\n",
      "Predicted Translation: Die Schere beim ihrer alten Dame .\n",
      "Actual Translation: Die Frau ihren Sohn , Drogen zu nehmen .\n",
      "\n",
      "Input Sentence: Have you informed everyone ?\n",
      "Predicted Translation: Hast du die Rechte gekannt ?\n",
      "Actual Translation: Haben Sie alle benachrichtigt ?\n",
      "\n",
      "Input Sentence: That is not something we need to worry about .\n",
      "Predicted Translation: Darüber sollte man nichts mehr machen .\n",
      "Actual Translation: Darum müssen wir uns keine Sorgen machen .\n",
      "\n",
      "Input Sentence: Tom lost his job .\n",
      "Predicted Translation: Tom hat seine Arbeit verloren .\n",
      "Actual Translation: Tom verlor seinen Arbeitsplatz .\n",
      "\n",
      "Input Sentence: I am not a criminal .\n",
      "Predicted Translation: Ich bin kein Kriminelle .\n",
      "Actual Translation: Ich bin kein Krimineller .\n",
      "\n",
      "Input Sentence: How did you celebrate your birthday ?\n",
      "Predicted Translation: Wie hast du dein Geburtstag ?\n",
      "Actual Translation: Wie hast du deinen Geburtstag gefeiert ?\n",
      "\n",
      "Input Sentence: I do not want to eat today .\n",
      "Predicted Translation: Ich heute nicht essen .\n",
      "Actual Translation: Ich will heute nichts essen .\n",
      "\n",
      "Input Sentence: What are the company strengths and weaknesses ?\n",
      "Predicted Translation: Was sind welche Firma und nahm ?\n",
      "Actual Translation: Was sind die und der Firma ?\n",
      "\n",
      "Input Sentence: I do not want to be seen .\n",
      "Predicted Translation: Ich will nicht , dass ich gesehen werden .\n",
      "Actual Translation: Ich will nicht gesehen werden .\n",
      "\n",
      "Input Sentence: What is Tom asking ?\n",
      "Predicted Translation: Was verlangt Tom ?\n",
      "Actual Translation: Um was bittet Tom ?\n",
      "\n",
      "Input Sentence: I can not go with you tonight .\n",
      "Predicted Translation: Ich kann heute Abend nicht mit dir gehen .\n",
      "Actual Translation: Ich kann heute Abend nicht mit dir gehen .\n",
      "\n",
      "Input Sentence: Tom girlfriend is really pretty .\n",
      "Predicted Translation: Toms Freundin ist ziemlich hübsch .\n",
      "Actual Translation: Toms Freundin ist .\n",
      "\n",
      "Input Sentence: I am working again .\n",
      "Predicted Translation: Ich habe jetzt wieder .\n",
      "Actual Translation: Ich arbeite wieder .\n",
      "\n",
      "Input Sentence: This costs more than that .\n",
      "Predicted Translation: Dieses kleiner hat mehr gekostet .\n",
      "Actual Translation: Das hier kostet mehr als das da .\n",
      "\n",
      "Input Sentence: I thought that it would make you laugh .\n",
      "Predicted Translation: Ich dachte , es würde euch lachen machen .\n",
      "Actual Translation: Ich dachte , es würde dich zum Lachen bringen .\n",
      "\n",
      "Input Sentence: Tom and Mary are not happy here .\n",
      "Predicted Translation: Tom und Maria sind hier nicht glücklich .\n",
      "Actual Translation: Tom und Maria fühlen sich hier nicht wohl .\n",
      "\n",
      "Input Sentence: How long have you been in Australia ?\n",
      "Predicted Translation: Wie lange bist du schon in Australien ?\n",
      "Actual Translation: Wie lange bist du schon in Australien ?\n",
      "\n",
      "Input Sentence: Tom spent the day drawing .\n",
      "Predicted Translation: Tom hat den Tag gefüttert .\n",
      "Actual Translation: Tom hat den ganzen Tag gezeichnet .\n",
      "\n",
      "Input Sentence: The man aimed a weapon at the police officers .\n",
      "Predicted Translation: Der Mann richtete sich die Polizei an der Bank .\n",
      "Actual Translation: Der Mann richtete eine Waffe auf den Polizisten .\n",
      "\n",
      "Input Sentence: What is this letter ?\n",
      "Predicted Translation: Was bedeutet dieser Brief ?\n",
      "Actual Translation: Was ist das für ein Buchstabe ?\n",
      "\n",
      "Input Sentence: It is still true today .\n",
      "Predicted Translation: Es ist noch immer noch warm .\n",
      "Actual Translation: Das stimmt noch heute .\n",
      "\n",
      "Input Sentence: Her father died .\n",
      "Predicted Translation: Ihr Vater ist gestorben .\n",
      "Actual Translation: Ihr Vater starb .\n",
      "\n",
      "Input Sentence: The same thing could have happened to you .\n",
      "Predicted Translation: Das , was du damit geschehen war .\n",
      "Actual Translation: Dir das gleiche passieren .\n",
      "\n",
      "Input Sentence: I am supporting you .\n",
      "Predicted Translation: Ich unterstütze euch .\n",
      "Actual Translation: Ich unterstütze Sie .\n",
      "\n",
      "Input Sentence: I can hardly stand it .\n",
      "Predicted Translation: Ich kann es kaum ertragen .\n",
      "Actual Translation: Ich kann es kaum aushalten .\n",
      "\n",
      "Input Sentence: Her novel was translated into Japanese .\n",
      "Predicted Translation: Ihr Roman wurde in Japanisch übersetzt .\n",
      "Actual Translation: Ihr Roman ist ins Japanische übersetzt worden .\n",
      "\n",
      "Input Sentence: Are you allergic to any plants ?\n",
      "Predicted Translation: Bist du irgendwelche Pflanzen allergisch ?\n",
      "Actual Translation: Seid ihr gegen irgendwelche Pflanzen allergisch ?\n",
      "\n",
      "Input Sentence: The movie was awful .\n",
      "Predicted Translation: Der Film war schrecklich .\n",
      "Actual Translation: Der Film war schrecklich .\n",
      "\n",
      "Input Sentence: Do not forget your umbrella .\n",
      "Predicted Translation: Vergiss nicht deinen Regenschirm .\n",
      "Actual Translation: Vergessen Sie nicht Ihren Regenschirm !\n",
      "\n",
      "Input Sentence: He does not earn enough money to live on .\n",
      "Predicted Translation: Er verdient nicht wegen des Geldes zu bezahlen .\n",
      "Actual Translation: Er verdient nicht genug Geld zum Leben .\n",
      "\n",
      "Input Sentence: We can not fail again .\n",
      "Predicted Translation: Wir nicht wieder wieder . .\n",
      "Actual Translation: Wir dürfen nicht noch einmal scheitern .\n",
      "\n",
      "Input Sentence: Were you born in a barn ?\n",
      "Predicted Translation: Bist du in einem Scheune geboren worden ?\n",
      "Actual Translation: Wurdest du in einer Scheune geboren ?\n",
      "\n",
      "Input Sentence: When was the last time you washed the car ?\n",
      "Predicted Translation: Wann hast du das letztemal abgewaschen ? ?\n",
      "Actual Translation: Wann haben Sie zuletzt das Auto gewaschen ?\n",
      "\n",
      "Input Sentence: Tom likes potatoes .\n",
      "Predicted Translation: Tom mag Kartoffeln .\n",
      "Actual Translation: Tom isst gerne Kartoffeln .\n",
      "\n",
      "Input Sentence: A taxi drew up to me .\n",
      "Predicted Translation: Ein Taxi hat mir zu jemand .\n",
      "Actual Translation: Ein Taxi fuhr auf mich zu .\n",
      "\n",
      "Input Sentence: He came back at five of the clock .\n",
      "Predicted Translation: Er kam um fünf Uhr auf .\n",
      "Actual Translation: Er kam um 5 Uhr zurück .\n",
      "\n",
      "Input Sentence: Tom asked Mary to help .\n",
      "Predicted Translation: Tom bat Maria , um zu helfen .\n",
      "Actual Translation: Tom bat Maria um Hilfe .\n",
      "\n",
      "Input Sentence: What is ours is ours .\n",
      "Predicted Translation: Was ist die Liebe unserer ?\n",
      "Actual Translation: Was uns , uns .\n",
      "\n",
      "Input Sentence: We lost the game .\n",
      "Predicted Translation: Wir haben das Spiel verloren .\n",
      "Actual Translation: Wir verloren das Spiel .\n",
      "\n",
      "Input Sentence: I did not know Tom was so lonely .\n",
      "Predicted Translation: Ich wusste nicht , dass Tom so einsam ist .\n",
      "Actual Translation: Ich wusste nicht , dass Tom so einsam ist .\n",
      "\n",
      "Input Sentence: Is this your suitcase ?\n",
      "Predicted Translation: Ist Ihr Ihr Koffer ? ?\n",
      "Actual Translation: Ist dieser Koffer deiner ?\n",
      "\n",
      "Input Sentence: Tom spent three years in prison .\n",
      "Predicted Translation: Tom verbrachte drei Jahre im .\n",
      "Actual Translation: Tom verbrachte drei Jahre im .\n",
      "\n",
      "Input Sentence: Tom is almost as tall as his father .\n",
      "Predicted Translation: Tom ist fast wie sein Vater .\n",
      "Actual Translation: Tom ist fast so wie sein Vater .\n",
      "\n",
      "Input Sentence: They were not pleased .\n",
      "Predicted Translation: Sie waren nicht erbaut .\n",
      "Actual Translation: Sie waren nicht erfreut .\n",
      "\n",
      "Input Sentence: Do not come out here .\n",
      "Predicted Translation: Komm nicht hier raus !\n",
      "Actual Translation: Komm nicht hier heraus .\n",
      "\n",
      "Input Sentence: Stop hiding your head in the sand .\n",
      "Predicted Translation: auf , deinen Platz zu einem geparkt ?\n",
      "Actual Translation: Lass dieses !\n",
      "\n",
      "Input Sentence: Who did you speak with ?\n",
      "Predicted Translation: Mit wem hast du gesprochen ?\n",
      "Actual Translation: Mit wem hast du gesprochen ?\n",
      "\n",
      "Input Sentence: I did not break my promise .\n",
      "Predicted Translation: Ich habe meine Versprechen nicht brechen .\n",
      "Actual Translation: Ich habe mein Versprechen nicht gebrochen .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import BLEUScore\n",
    "\n",
    "bleu_score = BLEUScore()\n",
    "\n",
    "device = next(seq2seq.model.parameters()).device\n",
    "for batch_index, (in_sequences, out_sequences) in enumerate(test_loader):\n",
    "    in_sentences = unprocess(in_sequences.to(device), in_vocab, specials)\n",
    "    pred_sequences = seq2seq.model.evaluate(in_sequences.to(device))\n",
    "    pred_sentences = unprocess(pred_sequences, out_vocab, specials)\n",
    "    out_sentences = unprocess(out_sequences.to(device), out_vocab, specials)\n",
    "\n",
    "    bleu_score.update(pred_sentences, [[s] for s in out_sentences])\n",
    "\n",
    "    print(f\"Input Sentence: {in_sentences[0]}\\n\"\n",
    "          + f\"Predicted Translation: {pred_sentences[0]}\\n\"\n",
    "          + f\"Actual Translation: {out_sentences[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU Score: 0.324\n"
     ]
    }
   ],
   "source": [
    "final_bleu = bleu_score.compute()\n",
    "print(f\"Validation BLEU Score: {final_bleu:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
